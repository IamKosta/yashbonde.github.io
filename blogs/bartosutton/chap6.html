<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">

    <title>Barto Sutton | Chapter 6 Notes</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:100,400,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Raleway:300,400,400i,600,600i,700,700i,800" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,400i,700,700i" rel="stylesheet">

    <!-- Link the main.css stylesheet -->
    <link href="../styles/reset.css" rel="stylesheet">
    <link href="../styles/-debug.css" rel="stylesheet">
    <link href="../styles/article.css" rel="stylesheet">
    <link href="../styles/article-text.css" rel="stylesheet">
    <link href="../styles/article-figure.css" rel="stylesheet">
    <link href="../styles/nav.css" rel="stylesheet">
    <link href="../styles/code-block.css" rel="stylesheet">
    <link href="./assets/exer.css" rel="stylesheet">

    <link href="../styles/footer-blog.css" rel="stylesheet">

    <!-- Latex -->
    <!-- for help with symbols goto: https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

</head>
<body>
    <nav>
        <a class = "nav-home" href="../../index.html">HOME</a>
        <a class = "nav-project" href="../../projects.html">PROJECTS</a>
        <a class = "nav-blog" href="../../musings.html">BLOG</a>
        <a class = "nav-resume" href="../../resume.html">RESUME</a>
    </nav>

    <article id = "blog-0">

        <h1>Chapter 6: Temporal Difference Learning [In-progress]</h1>
        <h2>Learning from Experiences minus the Complete Rollout</h2>

        <time datetime="19-05-2019">MAY 19, 2019</time>

        <p style="font-size: 0.8rem;">
            <h3>INDEX</h3>
            <li><a class="index-a" href="#6_1">6.1 TD Prediction</a></li>
            <li><a class="index-a" href="#6_2">6.2 Action Value Methods</a></li>
            <li><a class="index-a" href="#6_4">6.4 Incremental Implementation</a></li>
            <li><a class="index-a" href="#6_5">6.5 Tracking a non-stationary process</a></li>
            <li><a class="index-a" href="#6_5">6.6 Optimisitc Initial Values</a></li>
            <li><a class="index-a" href="#6_5">6.7 Upper Confidence Bound Action Selection</a></li>
            <li><a class="index-a" href="#6_5">6.8 Gradient Bandit Algorithm</a></li>
            <li><a class="index-a" href="#6_5">6.9 Associative Search (Contextual Bandits)</a></li>
        </p>

        <p>
            These are just my notes of the book <a href="http://incompleteideas.net/book/the-book-2nd.html" class="article-a">Reinforcement Learning: An Introduction</a>, all the credit for book goes to the authors and other contributors. My solutions to the exercises are on <a href="./chap5_ex.html" class="article-a">this page</a>. Addtional concise notes for this chapter <a href="https://drive.google.com/open?id=0B3w765rOKuKAcERUTmJaN1lJbUU" class="article-a">here</a>. There is also a <a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=4&t=0s" class="article-a">Deepmind's Youtube Lecture</a> and slides for <a href="./slides/L4_MC-TD.pdf" class="article-a">model free value extimation</a> and <a class="article-a" href="./slides/L5_Model_Free_control.pdf">model free control</a>. Some other <a href="https://drive.google.com/file/d/0B3w765rOKuKAZWV6bGFJcWN6VDg/view?usp=sharing" class="article-a">slides</a>.
        </p>

        <blockquote style="border-left: 8px solid #ffbf00;">
            We do not know what the rules of the game are; all we are allowed to do is watch the playing. Of course if we are allowed to watch long enough we may catch on a few rules. - Richard Feynman
        </blockquote>

        <h3 id="6_1">6.1 TD Prediction</h3>
        <p>
            From Monte-Carlo we know that value update formula is given as
            \[V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))\]
            however one limitation is that it depends upon the complete roll out of any episode, we can however calculate the estimated return for any state as \(G_t = R_{t+1} + \gamma V(S_{t+1})\) and replace it in the equation to get
            \[V(S_t) \leftarrow V(S_t) + \alpha(R_t + \gamma V(S_{t+1}) - V(S_t))\]
            We call \(R_t + \gamma V(S_{t+1})\) TD target and \(\delta_t = R_t + \gamma V(S_{t+1}) - V(S_t)\) the TD error. Since this is just one step look-ahead it is called TD(\(0\)) which is a special case of n-step TD and TD(\(\lambda\)). We use the idea of bootstrapping, that we start off with a guess make a move and based on the returns update our original guess.
        </p>

        <figure class="size-2">
            <img src="./assets/chap6_1.png">
        </figure>
        <figcaption><p style="text-align: center; font-size: 0.9rem;">Tabular TD(\(0\)) for estimating \(v_\pi\)</p></figcaption>
        <p>
            \[v_\pi(s) = \mathbb{E}[G_t|S_t = s]\]
            \[v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s]\]
            We can say that the upper equation is what the MC use as estimates and lower is what DP use as estimates for any state.
            Notice that the TD error \(\delta_t\) is the error made in estimate at that time. Because the TD error depends on the next state and next reward, it is not available until one time step later. We can calculate the Monte Carlo error as sum of TD errors from that point.
            \[G_t - V(S_t) = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) -\gamma V(S_{t+1}) \]
            \[= \delta_t + \gamma (G_{t+1} - V(S_{t+1}))\]
            we can now continue to substitute the values and see the expansion as
            \[= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T -V(S_T))\]
            \[= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0 - 0)\]
            \[\delta^{MC}_t = \sum_{k=t}^{T-t}\gamma^{k-t}\delta_k\]
            This identity is not exact if the \(V\) changes during the episode eg. an update, but if the step size is small it may still hold approximately. When \(V\) updates <a class="article-a" href="./chap6_ex.html#6_1">following</a> changes will have to be made.
        </p>
    </article>

    <footer>
        <p class="footer-p">
            CONNECT WITH ME
        </p>
        <div class = "share">
            <a href="https://www.linkedin.com/in/yash-bonde/"><img src="../images/linkedin.svg"></a>
            <a href="https://www.instagram.com/yassh.bonde/"><img src="../images/instagram.svg"></a>
        </div>
    </footer>

</body>
</html>